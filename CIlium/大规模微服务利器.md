Reference：https://arthurchiao.art/blog/ebpf-and-k8s-zh/

# Cilium eBPF流程

![img](https://arthurchiao.art/assets/img/ebpf-and-k8s/cilium-ebpf-1.png)

1. Cilium agent **生成 eBPF 程序**。
2. 用 LLVM 编译 eBPF 程序，**生成 eBPF 对象文件**（object file，`*.o`）。
3. 用 eBPF loader **将对象文件加载到 Linux 内核**。
4. 校验器（verifier）对 eBPF 指令会进行**合法性验证**，以确保程序是安全的，例如 ，无非法内存访问、不会 crash 内核、不会有无限循环等。
5. 对象文件被**即时编译（JIT）为能直接在底层平台**（例如 x86）运行的 native code。
6. 如果要在内核和用户态之间共享状态，BPF 程序可以使用 BPF map，这种一种**共享存储** ，BPF 侧和用户侧都可以访问。
7. **BPF 程序就绪，等待事件触发其执行**。对于这个例子，就是有数据包到达网络设备时，触发 BPF 程序的执行。
8. BPF 程序对收到的包进行处理，例如 mangle。最后**返回一个裁决**（verdict）结果。
9. 根据裁决结果，如果是 DROP，这个包将被丢弃；如果是 PASS，包会被送到更网络栈的 更上层继续处理；如果是重定向，就发送给其他设备。

# Kube-proxy包转发路径

![img](https://arthurchiao.art/assets/img/ebpf-and-k8s/flow-with-kube-proxy.png)

1. 网卡收到一个包（通过 DMA 放到 ring-buffer）。
2. 包经过 XDP hook 点。
3. 内核**给包分配内存**，此时才有了大家熟悉的 `skb`（包的内核结构体表示），然后 送到内核协议栈。
4. 包经过 GRO 处理，对分片包进行重组。
5. 包进入 tc（traffic control）的 ingress hook。接下来，**所有橙色的框都是 Netfilter 处理点**。
6. Netfilter：在 `PREROUTING` hook 点处理 `raw` table 里的 iptables 规则。
7. 包经过内核的**连接跟踪**（conntrack）模块。
8. Netfilter：在 `PREROUTING` hook 点处理 `mangle` table 的 iptables 规则。
9. Netfilter：在 `PREROUTING` hook 点处理 `nat` table 的 iptables 规则。
10. 进行**路由判断**（FIB：Forwarding Information Base，路由条目的内核表示，译者注） 。接下来又是四个 Netfilter 处理点。
11. Netfilter：在 `FORWARD` hook 点处理 `mangle` table 里的 iptables 规则。
12. Netfilter：在 `FORWARD` hook 点处理 `filter` table 里的 iptables 规则。
13. Netfilter：在 `POSTROUTING` hook 点处理 `mangle` table 里的 iptables 规则。
14. Netfilter：在 `POSTROUTING` hook 点处理 `nat` table 里的 iptables 规则。
15. 包到达 TC egress hook 点，会进行出方向（egress）的判断，例如判断这个包是到本 地设备，还是到主机外。
16. 对大包进行分片。根据 step 15 判断的结果，这个包接下来可能会：
17. 发送到一个本机 veth 设备，或者一个本机 service endpoint，
18. 或者，如果目的 IP 是主机外，就通过网卡发出去。

# Cilium eBPF 包转发路径

![img](https://arthurchiao.art/assets/img/ebpf-and-k8s/flow-with-cilium-ebpf-1.png)

对比可以看出，**Cilium eBPF datapath 做了短路处理**：从 tc ingress 直接 shortcut 到 tc egress，节省了 9 个中间步骤（总共 17 个）。更重要的是：这个 datapath **绕过了 整个 Netfilter 框架**（橘黄色的框们），Netfilter 在大流量情况下性能是很差的。

![img](https://arthurchiao.art/assets/img/ebpf-and-k8s/flow-with-cilium-ebpf-2.png)

**Cilium/eBPF 还能走的更远**。例如，如果包的目的端是另一台主机上的 service endpoint，那你可以直接在 XDP 框中完成包的重定向（收包 `1->2`，在步骤 `2` 中对包 进行修改，再通过 `2->1` 发送出去），将其发送出去，如下图所示：

![img](https://arthurchiao.art/assets/img/ebpf-and-k8s/flow-with-cilium-ebpf-3.png)

可以看到，这种情况下包都**没有进入内核协议栈（准确地说，都没有创建 skb）**就被转 发出去了，性能可想而知。





# Others

### kTLS & eBPF

> kTLS & eBPF for introspection and ability for in-kernel TLS policy enforcement

kTLS 是**将 TLS 处理 offload 到内核**，例如，将加解密过程从 openssl 下放到内核进 行，以**使得内核具备更强的可观测性**（gain visibility）。

有了 kTLS，就可以用 eBPF 查看数据和状态，在内核应用安全策略。 **目前 openssl 已经完全原生支持这个功能**。

### bpftool & libbpf

为了检查内核内 eBPF 的状态（introspection）、查看内核加载了哪些 BPF 程序等， 我们添加了一个新工具 bpftool。现在这个工具已经功能非常强大了。

同样，为了方便用户空间应用使用 eBPF，我们提供了**用户空间 API** （user space API for applications）**`libbpf`**。 这是一个 C 库，接管了所有加载工作，这样用户就不需要自己处理复杂的加载过程了。

### BTF（Byte Type Format）

内核添加了一个称为 BTF 的组件。这是一种元数据格式，和 DWARF 这样的 debugging data 类似。但 **BTF 的 size 要小的多**，更重要的是，有史以来 **内核第一次变得可自描述了**（self-descriptive）。什么意思？

想象一下当前正在运行中的内核，它**内置了自己的数据格式**（its own data format） 和**内部数据结构**（internal structures），你能用工具来查看这些东西（you can introspect them）。还是不太懂？这么说吧，**BTF 是后来的 “一次编译、到处运行”、 热补丁（live-patching）、BPF global data 处理等等所有这些 BPF 特性的基础**。

新的特性不断加入，它们都依赖 BTF 提供富元数据（rich metadata）这个基础。

### 新 socket 类型：AF_XDP

内核添加了一个**新 socket 类型 `AF_XDP`**。它提供的能力是： **在零拷贝（zero-copy）的前提下将包从网卡驱动送到用户空间**。

> 回忆前面的内容，数据包到达网卡后，先经过 XDP，然后才为这个包分配内存。 因此在 XDP 层直接将包送到用户态就绕过了内核内存分配和数据拷贝。 译者注

`AF_XDP` 提供的能力与 DPDK 有点类似，不过

- DPDK 需要**重写网卡驱动**，需要额外维护**用户空间的驱动代码**。
- `AF_XDP` 在**复用内核网卡驱动**的情况下，能达到与 DPDK 一样的性能。

而且由于**复用了内核基础设施，所有的网络管理工具还都是可以用的**，因此非常方便， 而 DPDK 这种 bypass 内核的方案导致绝大大部分现有工具都用不了了。

由于所有这些操作都是发生在 XDP 层的，因此它称为 `AF_XDP`。插入到这里的 BPF 代码 能**直接将包送到 socket**。

### bpffilter

开始了 bpffilter prototype，作用是通过用户空间驱动（userspace driver），**将 iptables 规则转换成 eBPF 代码**。

这是将 iptables 转换成 eBPF 的第一次尝试，整个过程对用户都是无感知的，其中的某些 组件现在还在用，用于在其他方面扩展内核的功能。



## Cilium 的Service LB设计

![img](https://arthurchiao.art/assets/img/ebpf-and-k8s/cilium-service-1.png)

如上图所示，主要涉及两部分：

1. 在 socket 层运行的 BPF 程序
2. 在 XDP 和 tc 层运行的 BPF 程序

### 东西向流量

我们先来看 socker 层。

![img](https://arthurchiao.art/assets/img/ebpf-and-k8s/cilium-service-2.png)

如上图所示，

**Socket 层的 BPF 程序主要处理 Cilium 节点的东西向流量**（E-W）。

- 将 Service 的 `IP:Port` 映射到具体的 backend pods，并做负载均衡。
- 当应用发起 **connect、sendmsg、recvmsg 等请求（系统调用）时，拦截这些请求**， 并根据请求的 `IP:Port` 映射到后端 pod，直接发送过去。反向进行相反的变换。

这里实现的好处：性能更高。

- **不需要包级别（packet leve）的地址转换**（NAT）。**在系统调用时，还没有创建包**，因此性能更高。
- 省去了 kube-proxy 路径中的很多中间节点（intermediate node hops）

可以看出，应用对这种拦截和重定向是无感知的（符合 k8s Service 的设计）。

### 南北向流量

再来看**从 k8s 集群外进入节点，或者从节点出 k8s 集群的流量（external traffic）， 即南北向流量（N-S）**：

> 区分集群外流量的一个原因是：Pod IP 很多情况下都是不可路由的（与跨主机选用的网 络方案有关），只在集群内有效，即，集群外访问 Pod IP 是不通的。
>
> 因此，如果 Pod 流量直接从 node 出宿主机，必须确保它能正常回来。而 node IP 一般都是全局可达的，集群外也可以访问，所以常见的解决方案就是：在 Pod 通过 node 出集群时，对其进行 SNAT，将源 IP 地址换成 node IP 地址；应答包回来时，再进行相 反的 DNAT，这样包就能回到 Pod 了。
>
> 译者注

![img](https://arthurchiao.art/assets/img/ebpf-and-k8s/cilium-service-3.png)

如上图所示，集群外来的流量到达 node 时，由 **XDP 和 tc 层的 BPF 程序进行处理**， 它们做的事情与 socket 层的差不多，将 Service 的 `IP:Port` 映射到后端的 `PodIP:Port`，如果 backend pod 不在本 node，就通过网络再发出去。发出去的流程我们 在前面 `Cilium eBPF 包转发路径` 讲过了。

这里 BPF 做的事情：执行 DNAT。**这个功能可以在 XDP 层做，也可以在 TC 层做**，但 在 XDP 层代价更小，性能也更高。

总结起来，这里的**核心理念**就是：

1. 将**东西向流量**放在**离 socket 层尽量近**的地方做。
2. 将**南北向流量**放在**离驱动（XDP 和 tc）层尽量近**的地方做。

# 未来展望

**“Linux 内核继续朝着成为 BPF runtime-powered microkernel 而前进”**。这是一个非 常有趣的思考角度。

- 设想在将来，Linux 只会保留一个非常小的核心内核（tiny core kernel），其他所有 内核功能都由用户定义，并用 BPF 实现（而不再是开发内核模块的方式）。
- 这样可以减少受攻击面，因为此时的核心内核非常小；另外，所有 BPF 代码都会经过 verifer 校验。
- 极大减少 ‘static’ feature creep，资源（例如 CPU）可以用在更有意义的地方。
- 设想一下，未来 Kubernetes 可能会内置 custom BPF-tailored extensions，能根据用户的应用自动适配（optimize needs for user workloads）；例如，判断 pod 是跑在数据中心，还是在嵌入式系统上。